---
layout: single
title:  "Real-time AI Vision System: Object Detection. #AWS #NVIDIA Jetson #ROS2 #TensorRT"
date:   2022-10-20 00:43:00 -0500
collection: projects
author_profile: true
toc: true
classes: wide
header:
    teaser: /assets/images/project_OD/OD.png
---

This is my personal project where I trained a YOLO v4 model on the COCO dataset using an AWS EC2 cloud server, and successfully deployed the YOLO object detection model on an NVIDIA Jetson AGX Xavier platform powered by ROS2 and TensorRT C, delivering 65 frames per second with an input resolution of 512x512 RGB images (resized from 1080p).

# Setup
- AWS access for EC2 (p3.2xlarge)

- NVIDIA Jetson AGX Xavier (JetPack 4.5)

- ROS2 installed (Eloquent)

- Logitech C920 Webcam HD Pro

- A monitor (HDMI)

# Workflow
The training phase was executed on an AWS EC2 cloud server utilizing PyTorch. An Onnx file was utilized as a connector to transfer the trained model to the deployment hardware. The deployment phase was performed on an NVIDIA Jetson AGX Xavier using ROS2 and TensorRT C, as the C programming language was selected for its exceptional speed performance in comparison to Python.

<style>
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  min-width: 90%;
  max-width: 90%;
  width: 50vw;
}
</style>
<img class="center" src="/assets/images/project_OD/Task_1.png" alt="The task."> 

# Demo
Here is a picture of my setup with a NVIDIA Jetson AGX Xavier, Logitech C920 Webcam and a monitor.
<style>
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  min-width: 90%;
  max-width: 90%;
  width: 50vw;
}
</style>
<img class="center" src="/assets/images/project_OD/Setup.png" alt="Setup."> 
I utilized a Satechi 5-Port USB-C Hub to increase the number of USB-A and USB-C ports on the NVIDIA Jetson AGX Xavier. This allowed me to connect my external keyboard, mouse, and camera. The Jtop, which is similar to nvidia-smi, was utilized to monitor the utilization of the GPU. The Logitech C920 Webcam was mounted on a tripod, so it could stand on its own.


Here are two screenshots of the visualization node that display the results of the object detection.

</style>
<img class="center" src="/assets/images/project_OD/test_1.png" alt="test_1."> 

On the upper-right corner of the bounding box, the first parameter displayed is the current fps of the TensorRT ROS2 node. The second parameter indicates the object class, and the last parameter represents the probability that the object belongs to that class.

</style>
<img class="center" src="/assets/images/project_OD/test_2.png" alt="test_2."> 

Overall, I am satisfied with the performance of this setup, which runs at a rate of 55-65 fps on the NVIDIA Jetson AGX Xavier. This setup has the potential for further optimization as I am currently using JetPack 4.5 which includes TensorRT 7. By upgrading to JetPack with TensorRT 8, which supports end-to-end ONNX models with NMS plugins in the object detection model, it could enhance the fps performance on edge hardware.
