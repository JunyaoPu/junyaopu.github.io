---
layout: single
title:  "Real-time AI Vision System: Object Detection."
date:   2022-05-15 00:43:00 -0500
collection: projects
author_profile: true
toc: true
classes: wide
header:
    teaser: /assets/images/project_OD/test_1.png
---

This is my personal project where I trained a YOLO (v4) model on the COCO dataset using an AWS EC2 cloud server, and deployed the YOLO object detection model on an NVIDIA Jetson AGX Xavier hardware with ROS2 C/C++ and TensorRT C/C++, delivering 68 frames per second with input RGB images from a Logitech webcam.

# Setup
- AWS access for EC2 (p3.2xlarge)

- NVIDIA Jetson AGX Xavier (JetPack 4.5)

- ROS2 installed (Eloquent)

- Logitech C920 Webcam HD Pro

- A monitor 

# Workflow
The training phase was executed on an AWS EC2 cloud server utilizing PyTorch. An Onnx file was utilized as a connector to transfer the trained model to the deployment hardware. The deployment phase was performed on an NVIDIA Jetson AGX Xavier using ROS2 and TensorRT, as the C/C++ programming language was selected for its exceptional speed for input pre-processing and output post-processing.

<style>
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  min-width: 90%;
  max-width: 90%;
  width: 50vw;
}
</style>
<img class="center" src="/assets/images/project_OD/Task_1.png" alt="The task."> 

# Demo
Here is a picture of my setup with a NVIDIA Jetson AGX Xavier, Logitech C920 Webcam and a monitor. 
<style>
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  min-width: 90%;
  max-width: 90%;
  width: 50vw;
}
</style>
<img class="center" src="/assets/images/project_OD/Setup.png" alt="Setup."> 
I used a Satechi USB-C Hub to expand the number of USB ports on the NVIDIA Jetson AGX Xavier, enabling me to connect my external keyboard, mouse, and camera. The Logitech C920 Webcam was mounted on a tripod, so it could stand on its own. I was running three ROS2 nodes, including a camera node, a TensorRT node, and a visualization node. In addition, I used another terminal with the 'jtop' tool, which is similar to 'nvidia-smi', to monitor the GPU usage. 


Here are two screenshots from the visualization node that show the results of the object detection.

<img class="center" src="/assets/images/project_OD/test_1.png" alt="test_1."> 

On the upper-left corner of the bounding box, the first parameter displayed is the current fps of the TensorRT ROS2 node. The second parameter indicates the object class, and the last parameter represents the probability that the object belongs to that class.

<img class="center" src="/assets/images/project_OD/test_2.png" alt="test_2."> 

Overall, I am satisfied with the performance of this setup, which runs at a rate of 55-68 fps on the NVIDIA Jetson AGX Xavier. This setup has the potential for further optimization as I am currently using JetPack 4.5 which includes TensorRT 7. By upgrading to JetPack with TensorRT 8, which supports end-to-end ONNX models with NMS plugins in the object detection model, it could enhance the fps performance on NVIDIA edge hardware.
